---
title: Bioconda and Docker containers for bioinformatics
author: "CG"
date: "2024-11-01"
date-modified: "2024-11-01"
categories: [docker, bioinformatics, containerization]
engine: knitr
---

This post is intended to give a short practical example of using two key tools to install bioinformatics tools and use them.  Both conda and docker are widely used.  I have used conda for a little under a decade and only really adopted docker a few years ago, but I can see that containerization is more and more widely used.  Hopefully this post can serve as a practical example for those experimenting with these approaches. 

I find tools like conda, pyenv, pip, venv, rig, and renv are all excellent for environment management.  They all have a learning curve, but quickly become part of your day-to-day tooling.  However, the problem remains that sometimes you are at the start of a new project, or worse halfway through an existing project and you need to install a new tool - this new tool may have new dependencies and in the latter case, may well share dependencies with existing tools, potentially with different versions.  The unfortunate truth of the matter is many bioinformaticians spend an unreasonable amount of time getting their tooling setup.

There are many solutions to this - one I have seen many resort to, is to have a conda environment per tool, to ensure dependencies are handled independently.  Whether you use a separate environment for each tool or not, using conda is effective and I will give some basic commands to get started in the [first part](#using-bioconda-to-install-bcftools-and-plink) of this post.  An alternative approach is to use pre-built docker containers, which I will demonstrate in the [second half](#docker-and-biocontainers-running-bcftools-as-a-container) of this post.

## Using Bioconda to install bcftools


The [bioconda](https://bioconda.github.io/) channel contains lots of useful tooling for bioinformatics, meaning it's possible to use conda to manage some bioinformatics tooling directly.  To add the bioconda channel to you conda setup, the bioconda documentation suggests to run the following commands to update your ~/.condarc file.


1. Add bioconda to the conda channels

```{bash}
#| eval: false
    conda config --add channels bioconda
    conda config --add channels conda-forge
    conda config --set channel_priority strict
```


```{bash}
#| eval: false
    # list activate channels as follows
    conda config --show channels
```

2. Create and activate your conda environment as usual

```{bash}
#| eval: false
    conda create -n bcftools-env -y
    conda activate bcftools-env
```
3. install bcftools 

```{bash}
#| eval: false
    conda install -c bioconda bcftools -y
```

4. run bcftools
```{bash}
#| eval: false
    bcftools --version
```

5. Once you have a working environment that you can carry out your analysis in, don't forget to export the environment yml
```{bash}
#| eval: false
    conda env export > environment.yml
```

Bioconda has gone a long way to remove the overhead of managing all the environment dependencies, understanding the dependencies of all your tools and compiling the source code.  I have used the above setup on my personal notebook and also on remote compute environments when working in the cloud.  However, conda environments are somewhat slow to setup (admittedly of the order of minutes, or at worst hours, not  days, usually)

### Reproducing your results: recreating your conda environment

Note: if you, or a colleague/collaborator later want to recreate an environment to rerun an analysis, you can do so using the environment.yml. The following command: `conda env create -f environment.yml` will recreate the environment.  This is a really powerful tool for creating portable environments, and enabling reproducible research.

## Docker and biocontainers: running bcftools as a container

In some cases tools are already available from public container repositories - a good example of this is bcftools in the [biocontainers](https://biocontainers.pro/) container registry.  After selecting a docker container version from the [bcftools page](https://biocontainers.pro/tools/bcftools), it can be run as follows (we have chosen the image `quay.io/biocontainers/bcftools:1.21--h8b25389_0`):

```{bash}
#| eval: false
docker pull quay.io/biocontainers/bcftools:1.21--h8b25389_0
docker run quay.io/biocontainers/bcftools:1.21--h8b25389_0 bcftools --version
```

Note that after the `docker run <image>`, the command starts that you want to run in the shell.  This is a common pattern.

If you are familiar with docker and containerization then you will be aware that this has executed a command in a container on your local machine.  Note that machine resources do not automatically scale to your system, there are usually limits enforced by the docker client.  You will need to manage allowed resources, for instance memory limits, via the docker client on your machine to ensure that enough memory is made available (for Mac and Windows you can do this via the docker desktop client).

To make use of this tooling then, we need to ensure the input and output directories are mounted to the container.  This is done with a `-v local-path:container-path` argument to the container.

In its simplest form, to run a docker container on the current location, we can mount the working directory as follows:

```{bash}
#| eval: false
docker run -v "${PWD}:/tmp/" quay.io/biocontainers/bcftools:1.21--h8b25389_0 ls /tmp/
```

Note that this is not running bcftools, it is running the command `ls /tmp/` on an isolated (containerized) linux OS running on the local machine.  If you run this command you will see a list of the files in the current working directory.  Since bcftools is installed and in the path on the container, you can run bcftools on files in your working directory as below.  In this example, we list the samples in a vcf file in the current working directory.

```{bash}
#| eval: false
docker run \
    -v "${PWD}:/tmp/" \
    quay.io/biocontainers/bcftools:1.21--h8b25389_0 \
    bcftools query -l /tmp/test-file-chr22.vcf.gz > /tmp/sample-id-list.txt
```

The command `bcftools query -l /tmp/test-file-chr22.vcf.gz > /tmp/sample-id-list.txt` will have written the list of sample IDs to a file `sample-id-list.txt` that now exists on your machine in the current working directory.

Now, `${PWD}` contains the path to you working directory (if you're not familiar with this, type `echo "${PWD}"` into the command line and it will print the path to your current directory, usually achieved with `pwd`).   We can thus generalize the above to mount any directory to the docker container.  If you wanted to mount an input and an output directory, you could do this as follows:

```{bash}
#| eval: false
docker run \
    -v "/path/to/input/directory:/tmp/input/" \
    -v "/path/to/output/directory:/tmp/output/" \
    quay.io/biocontainers/bcftools:1.21--h8b25389_0 \
    bcftools query -l /tmp/input/test-file-chr22.vcf.gz > /tmp/output/sample-id-list.txt
```

Notice that we have mounted the input directory to /tmp/input and the output directory to /tmp/output.  The use of the directories in the bcftools command that is executed is updated accordingly.

### Reproducing your results:

One major advantage with using docker containers is that you can simply rerun the commands to pull the same image and run the same commands (make sure you script the commands you use).