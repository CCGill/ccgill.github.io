---
title: Bioconda and Biocontainers Docker containers for bioinformatics
author: "CG"
date: "2024-11-01"
date-modified: "2024-11-01"
categories: [docker, bioinformatics, containerization]
engine: knitr
---

This post is intended to give a short practical example of using two widely used methods, conda and docker, to use bioinformatics tools while avoiding much of the overhead of installing and compiling them.  I have used conda for a little under a decade and got started with docker a few years ago.  Hopefully this post can serve as a practical example for those getting started with these approaches.  I've included docker here as it is extremely efficient, containerization is more and more widely used, and integrated into many workflow management tools [^footnote1].

[^footnote1]: Workflow management tools will be the subject of a future post - I will likely post on [nextflow](https://www.nextflow.io/) and [snakemake](https://snakemake.readthedocs.io/en/stable/))

I find tools like conda, pyenv, pip, venv, rig, and renv are all excellent for environment management.  They all have strengths, limitations, and a learning curve, but quickly become part of your day-to-day tooling.  However, the problem remains that sometimes you are at the start of a new project, or worse halfway through an existing project and you need to install a new tool - this new tool may have new dependencies, require compilation, and may well share dependencies with existing tools, potentially with different versions.

There are many solutions to this - one I have seen several resort to is a conda environment per tool, to ensure dependencies are handled independently (note, this can be difficult to manage effectively and collaborate with - the overhead for setting up a working environment increases significantly).  Whether you use a separate environment for each tool or not, using conda is effective and I will give some basic commands to get started in the [first part](#using-bioconda-to-install-bcftools-and-plink) of this post.  An alternative approach is to use pre-built docker containers, which I will demonstrate in the [second half](#docker-and-biocontainers-running-bcftools-as-a-container) of this post.

## Using Bioconda to install bcftools

The [bioconda](https://bioconda.github.io/) channel contains lots of useful tooling for bioinformatics, meaning it's possible to use conda to manage some bioinformatics tooling directly.  

### Add bioconda to the conda channels

To add the bioconda channel to you conda setup, the bioconda documentation suggests to run step 1 of the below the following commands to update your ~/.condarc file.

```{bash}
#| eval: false
    conda config --add channels bioconda
    conda config --add channels conda-forge
    conda config --set channel_priority strict
```


```{bash}
#| eval: false
    # list activate channels as follows
    conda config --show channels
```

### Installing and running bcftools 

1. Create and activate your conda environment as usual


```{bash}
#| eval: false
    conda create -n bcftools-env -y
    conda activate bcftools-env
```

2. install bcftools 

```{bash}
#| eval: false
    conda install -c bioconda bcftools -y
```

3. run bcftools

```{bash}
#| eval: false
    bcftools --version
```

4. Once you have a working environment that you can carry out your analysis in, don't forget to export the environment yaml file.  This is necessary to ensure you can recreate the environment yourself.

```{bash}
#| eval: false
    conda env export > environment.yml
```

Bioconda has removed a lot of the overhead of managing all the environment dependencies, compiling various tools, understanding the dependencies of all your tools and often needing to compile the source code.  I have used the above setup on my personal notebook and also on remote compute environments when working in the cloud.  In comparison to the past manual setup and management, this is a much better user experience.  However, conda environments are somewhat slow to setup, in comparison to containerized solutions (see later).

### Reproducing your results: recreating your conda environment

Note: if you, or a colleague/collaborator, later want to recreate an environment to rerun an analysis, you can do so using the environment.yml. The following command: `conda env create -f environment.yml` will recreate the environment.  This is a really powerful tool for creating portable environments, and enabling reproducible research.

## Docker and biocontainers: running bcftools as a container

In some cases tools are already available from public container repositories - a good example of this is bcftools in the [biocontainers](https://biocontainers.pro/) container registry - if you read the documentation you will see that biocontainers is tightly tied to bioconda, but I'll leave that to the reader to look into and here I'll focus on the practical steps to use the tool.  

After selecting a docker container version from the [bcftools page](https://biocontainers.pro/tools/bcftools), it can be run as follows (we have chosen the image `quay.io/biocontainers/bcftools:1.21--h8b25389_0`):

```{bash}
#| eval: false
docker pull quay.io/biocontainers/bcftools:1.21--h8b25389_0
docker run quay.io/biocontainers/bcftools:1.21--h8b25389_0 bcftools --version
```

Note that after the `docker run <image>`, the command starts that you want to run in the shell.  This is a common pattern.

If you are familiar with docker and containerization then you will be aware that this has executed a command in an isolated container on your local machine.  Container resources do not automatically scale to your system resources, there are usually limits enforced by the docker client.  You will need to manage allowed resources, for instance memory limits, via the docker client on your machine to ensure that enough memory is made available (for Mac and Windows you can do this via the docker desktop client).

To make use of this tooling then, we need to ensure the input and output directories are mounted to the container.  This is done with a `-v local-path:container-path` argument to the container, mounting the local directory (specified here as `local-path` to the directory in the container `container-path`).

In its simplest form, to run a command in a docker container on the current working directory, we can mount the working directory as follows:

```{bash}
#| eval: false
docker run -v "${PWD}:/tmp/" quay.io/biocontainers/bcftools:1.21--h8b25389_0 ls /tmp/
```

Note that this is not running bcftools, it is running the command `ls /tmp/` on an isolated (containerized) linux OS running on the local machine.  If you run this command you will see a list of the files in the current working directory.  Since bcftools is installed and in the path on the container, you can run bcftools on files in your working directory as below.  In this example, we list the samples in a vcf file in the current working directory.

```{bash}
#| eval: false
docker run \
    -v "${PWD}:/tmp/" \
    quay.io/biocontainers/bcftools:1.21--h8b25389_0 \
    bcftools query -l /tmp/test-file-chr22.vcf.gz > /tmp/sample-id-list.txt
```

The command `bcftools query -l /tmp/test-file-chr22.vcf.gz > /tmp/sample-id-list.txt` will have written the list of sample IDs to a file `sample-id-list.txt` to the `/tmp/` directory on the container, so that now exists on your machine in the current working directory.

Now, to understand this a little better: `${PWD}` contains the path to you working directory (if you're not familiar with this, type `echo "${PWD}"` into the command line and it will print the path to your current directory, usually achieved with `pwd`).   We can thus generalize the above to mount any directory to the docker container.  If you wanted to mount an input and an output directory, you could do this as follows:

```{bash}
#| eval: false
docker run \
    -v "/path/to/input/directory:/tmp/input/" \
    -v "/path/to/output/directory:/tmp/output/" \
    quay.io/biocontainers/bcftools:1.21--h8b25389_0 \
    bcftools query -l /tmp/input/test-file-chr22.vcf.gz > /tmp/output/sample-id-list.txt
```

Notice that we have mounted the input directory to /tmp/input and the output directory to /tmp/output.  These are updated accordingly in the bcftools command that is being run on the container.

### Reproducing your results:

One major advantage of using docker containers is that you can simply rerun the commands to pull the same image and run the same commands.  Two key points stand out:

- make sure you script the commands you use, and 
- be wary of the `latest` tag: this is great for exploratory work but, as the name suggests, points to the latest version of the container image, and this will change over time.