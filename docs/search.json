[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nThis is the first time I have returned to blogging in some years, and I look forward to it. I hope to repost some of my old work as well as create new material. The posts are at times notes for myself, with tools I’ve developed or used, as well as opinions, pedagogical posts, tips, tricks, and references. I have learnt a great deal from reading other blogs, so I hope the posts generated in this blog are of similar help to others."
  },
  {
    "objectID": "posts/007-reading-large-datasets-with-arrow/007-basic-intro-reading-large-datasets.html",
    "href": "posts/007-reading-large-datasets-with-arrow/007-basic-intro-reading-large-datasets.html",
    "title": "Working with large datasets with Arrow",
    "section": "",
    "text": "It is becoming more and more common to work with large datasets - since large seems to change frequently, I’ll be more specific: data that is too large to read into memory, or for which the I/O time is too large to rely on in any reasonable time-frame for the intended use.\nMost long-term R users I know seem to fall into two camps - they either apply base R’s read.table/read.csv or use readr’s offerings. The latter offerings are an impressive suite of tools that brought improvements and a helpful UI by comparison with the base R offerings, and ones that I usually recommend to newer users1.\nWhen faced with data that cannot be read into memory, or for which the I/O time is intolerably large, there are a few options: In the past I have relied on relatively complex workflows to process data in parallel chunks and subsequently aggregated it. Over the last few years, however, I have found the Arrow data structures and computational engine to be a game-changer. The performance gains are immense when reading data into an arrow structure and processing it, or (more often) from “lazily” loading the data and running optimized queries. As importantly, these have been made readily available to R users familiar with the dplyr syntax, via the implementation of the dplyr backend in the arrow package. The aim of this post is to give a very simple basic example to demonstrate using it, and also address a workaround I have found useful recently."
  },
  {
    "objectID": "posts/007-reading-large-datasets-with-arrow/007-basic-intro-reading-large-datasets.html#tips",
    "href": "posts/007-reading-large-datasets-with-arrow/007-basic-intro-reading-large-datasets.html#tips",
    "title": "Working with large datasets with Arrow",
    "section": "Tips",
    "text": "Tips\n\nSelect early\nTooling such as the arrow package and polars get huge performance boosts by optimizing queries and reading only the data required. If you can structure your queries to provide the compiler with information this can provide huge speed improvements. A good practice is to select only the columns you need, as early as possible - this ensures that only the relevant fields are read from the file, and can lead to significant performance gains.\n\n\nAvoid Data Type Surprises\nThis tip can be summed up as data types matter - unless the raw data is validated well, automated type-casting to specific types can cause data-loss and/or invalidate downstream processing steps. Whether you think data types matter when you’re running analyses or not probably depends most on what stage of the workflow you’re at and the level of confidence you have in the data. For untrusted data it is sometimes worthwhile exploring and validating the data as strings, and once validated, casting to a more appropriate or memory efficient data type, and store in a compressed, typed, data storage (such as parquet2) ready for downstream analysis.\nSo, datatypes matter. Attentive readers will have noticed that each of the columns above have a datatype assigned to them and these are not basic R datatypes. This is because the computation is being carried out using Arrow’s well-optimized datatypes and storage (see the arrow documentation and cookbook for further discussion on arrow types).\nSince parquet files are so widely used these days for large data, and in this case the data is stored with a type, it is perfectly possible that the datatypes are correct and this is convenient. However, for csv files 3, they are inferred by partially reading the file and this can cause some errors to be thrown when running queries (this is somewhat orthogonal to data-loss mentioned above). Fair warning, although most of the time the errors thrown by the arrow package are descriptive, I have occasionally found an error that was completely unclear but was fixed in exactly the same way: fixing the schema at read time, and in the worst case, reading all the data as string.\nTo demonstrate this an example where the type inference fails, we regenerate the above dataset wiht a column that includes a large number of missing values before some integers, and perform a count operation.\n\nfor (ind in seq(1,100)){\n  fname &lt;- glue::glue(\"iris-{ind}.csv\")\n  \n  iris |&gt; \n    dplyr::mutate(\n      data_version = ind,\n      badly_typed_column = `if`(ind &gt; 99, 1L, NA)\n      ) |&gt; \n    readr::write_csv(fs::path(working_dir, fname))\n}\n\niris_data &lt;- arrow::open_csv_dataset(sources = working_dir)\niris_data\n\nFileSystemDataset with 100 csv files\n7 columns\nSepal.Length: double\nSepal.Width: double\nPetal.Length: double\nPetal.Width: double\nSpecies: string\ndata_version: int64\nbadly_typed_column: null\n\n\nAs we see from the above output the badly_typed_column has a datatype of null. This can occur in situations as above, in the first 99 files the badly_typed_column contains only NA’s, while for the 100th file, it contains 1’s.\n\nquery &lt;- iris_data |&gt; dplyr::count(badly_typed_column, data_version)\n\nquery\n\nFileSystemDataset (query)\nbadly_typed_column: null\ndata_version: int64\nn: int64\n\nSee $.data for the source Arrow object\n\n\n\n# wrapped in try-catch to enable quarto to render document.\ntryCatch(\n  {query |&gt; dplyr::collect()},\n  error=\\(e) {message('An Error Occurred'); print(e)}\n)\n\nAn Error Occurred\n\n\n&lt;error/rlang_error&gt;\nError in `compute.arrow_dplyr_query()` at arrow/R/dplyr-collect.R:22:3:\n! Invalid: Could not open CSV input source '/tmp/RtmpCmGWnp/file56ea4287743d/iris/iris-100.csv': Invalid: In CSV column #6: Row #2: CSV conversion error to null: invalid value '1'\nℹ If you have supplied a schema and your data contains a header row, you should supply the argument `skip = 1` to prevent the header being read in as data.\n---\nBacktrace:\n    ▆\n 1. ├─base::tryCatch(...)\n 2. │ └─base (local) tryCatchList(expr, classes, parentenv, handlers)\n 3. │   └─base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]])\n 4. │     └─base (local) doTryCatch(return(expr), name, parentenv, handler)\n 5. ├─dplyr::collect(query)\n 6. └─arrow:::collect.arrow_dplyr_query(query) at dplyr/R/compute-collect.R:51:3\n 7.   └─arrow:::compute.arrow_dplyr_query(x) at arrow/R/dplyr-collect.R:22:3\n\n\nAs we can see, the error is pretty clear here, the inferred type was not consistent with the data in file 100. This caused a failure. The solution is clear - you must specify a schema for the data, by creating a schema object at dataset creation time. In this case, the below would suffice:\n\nlibrary(arrow, warn.conflicts = F)\n\ntyped_iris_data &lt;- open_csv_dataset(\n  sources = working_dir, \n  schema = schema(\n    field(\"Sepal.Length\", double()),\n    field(\"Sepal.Width\", float64()),\n    field(\"Petal.Length\", float64()),\n    field(\"Petal.Width\", float64()),\n    field(\"Species\", string()),\n    field(\"data_version\", int64()),\n    field(\"badly_typed_column\", int32())\n  ),\n  skip=1 # this is required to avoid reading the headers if a schema is provided and reading multiple files.\n)\n\nRunning the query will now succeed:\n\ntyped_iris_data |&gt; \n  dplyr::count(badly_typed_column, is.na(data_version)) |&gt; \n  dplyr::collect()\n\n# A tibble: 2 × 3\n  badly_typed_column `is.na(data_version)`     n\n               &lt;int&gt; &lt;lgl&gt;                 &lt;int&gt;\n1                 NA FALSE                 14850\n2                  1 FALSE                   150"
  },
  {
    "objectID": "posts/007-reading-large-datasets-with-arrow/007-basic-intro-reading-large-datasets.html#read-all-columns-as-strings",
    "href": "posts/007-reading-large-datasets-with-arrow/007-basic-intro-reading-large-datasets.html#read-all-columns-as-strings",
    "title": "Working with large datasets with Arrow",
    "section": "Read all columns as strings",
    "text": "Read all columns as strings\nAs I’ve found frequently the case (perhaps surprisingly often, though this may depend on the field one works in) if you’re quickly exploring data, it is sometimes convenient to read the data as strings. The below function is a convenient override for doing exactly this.\n\nopen_dataset_all_str &lt;- function(sources){\n  # 1. get column names from dataset\n  typed_dataset &lt;- arrow::open_csv_dataset(sources = sources)  \n  columns &lt;- names(typed_dataset)\n  # 2. create custom schema\n  schema &lt;- arrow::schema(lapply(columns, \\(x){arrow::field(x,arrow::string())}))\n  # 3. read dataset with custom schema\n  dataset &lt;- arrow::open_csv_dataset(sources = sources, schema=schema, skip=1)\n  # Note skip=1 here, to avoid reading in the header lines from multiple files as entries in the columns.\n  return(dataset)\n}\n\n\ntyped_dt &lt;- open_csv_dataset(sources=working_dir)\ntyped_dt\n\nFileSystemDataset with 100 csv files\n7 columns\nSepal.Length: double\nSepal.Width: double\nPetal.Length: double\nPetal.Width: double\nSpecies: string\ndata_version: int64\nbadly_typed_column: null\n\n\n\nstr_dt &lt;- open_dataset_all_str(sources=working_dir)\nstr_dt\n\nFileSystemDataset with 100 csv files\n7 columns\nSepal.Length: string\nSepal.Width: string\nPetal.Length: string\nPetal.Width: string\nSpecies: string\ndata_version: string\nbadly_typed_column: string\n\n\n\nunlink(working_dir,recursive=T)"
  },
  {
    "objectID": "posts/007-reading-large-datasets-with-arrow/007-basic-intro-reading-large-datasets.html#footnotes",
    "href": "posts/007-reading-large-datasets-with-arrow/007-basic-intro-reading-large-datasets.html#footnotes",
    "title": "Working with large datasets with Arrow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPreviously I have recommended data.table’s fread (and fwrite), but I found that new users were often confused when they stumbled across the differences between a data.table and a data.frame or tibble. The data.table tooling is highly effective, I’ve used it for manipulations of genomic data that simply weren’t possible with less capable libraries, but there are differences to the data.frame that can silently cause issues in pipelines if not accounted for.↩︎\nFor the base R users, and those who care less about cross-platform integrations, I still occasionally use RDS and in the past have used feather, particularly if storing a list of objects, but for dataframes I’ve found it is very difficult to beat parquet, particularly in teams using multiple languages/frameworks.↩︎\nYes - I know: Yes, for small enough csv files there should be no need to process them like this and if there are enough rows to justify this approach then arguably the data has no business being stored in a csv file. However, in my experience I’ve had to sort this issue for myself and for numerous other people when supporting legacy systems or client requirements.↩︎"
  },
  {
    "objectID": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html",
    "href": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html",
    "title": "Bioconda and Biocontainers Docker containers for bioinformatics",
    "section": "",
    "text": "This post is intended to give a short practical example of using two widely used methods, conda and docker, to use bioinformatics tools while avoiding much of the overhead of installing and compiling them. I have used conda for a little under a decade and got started with docker a few years ago. Hopefully this post can serve as a practical example for those getting started with these approaches. I’ve included docker here as it is extremely efficient, containerization is more and more widely used, and integrated into many workflow management tools 1.\nI find tools like conda, pyenv, pip, venv, rig, and renv are all excellent for environment management. They all have strengths, limitations, and a learning curve, but quickly become part of your day-to-day tooling. However, the problem remains that sometimes you are at the start of a new project, or worse halfway through an existing project and you need to install a new tool - this new tool may have new dependencies, require compilation, and may well share dependencies with existing tools, potentially with different versions.\nThere are many solutions to this - one I have seen several resort to is a conda environment per tool, to ensure dependencies are handled independently (note, this can be difficult to manage effectively and collaborate with - the overhead for setting up a working environment increases significantly). Whether you use a separate environment for each tool or not, using conda is effective and I will give some basic commands to get started in the first part of this post. An alternative approach is to use pre-built docker containers, which I will demonstrate in the second half of this post."
  },
  {
    "objectID": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#using-bioconda-to-install-bcftools",
    "href": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#using-bioconda-to-install-bcftools",
    "title": "Bioconda and Biocontainers Docker containers for bioinformatics",
    "section": "Using Bioconda to install bcftools",
    "text": "Using Bioconda to install bcftools\nThe bioconda channel contains lots of useful tooling for bioinformatics, meaning it’s possible to use conda to manage some bioinformatics tooling directly.\n\nAdd bioconda to the conda channels\nTo add the bioconda channel to you conda setup, the bioconda documentation suggests to run step 1 of the below the following commands to update your ~/.condarc file.\n\n    conda config --add channels bioconda\n    conda config --add channels conda-forge\n    conda config --set channel_priority strict\n\n\n    # list activate channels as follows\n    conda config --show channels\n\n\n\nInstalling and running bcftools\n\nCreate and activate your conda environment as usual\n\n\n    conda create -n bcftools-env -y\n    conda activate bcftools-env\n\n\ninstall bcftools\n\n\n    conda install -c bioconda bcftools -y\n\n\nrun bcftools\n\n\n    bcftools --version\n\n\nOnce you have a working environment that you can carry out your analysis in, don’t forget to export the environment yaml file. This is necessary to ensure you can recreate the environment yourself.\n\n\n    conda env export &gt; environment.yml\n\nBioconda has removed a lot of the overhead of managing all the environment dependencies, compiling various tools, understanding the dependencies of all your tools and often needing to compile the source code. I have used the above setup on my personal notebook and also on remote compute environments when working in the cloud. In comparison to the past manual setup and management, this is a much better user experience. However, conda environments are somewhat slow to setup, in comparison to containerized solutions (see later).\n\n\nReproducing your results: recreating your conda environment\nNote: if you, or a colleague/collaborator, later want to recreate an environment to rerun an analysis, you can do so using the environment.yml. The following command: conda env create -f environment.yml will recreate the environment. This is a really powerful tool for creating portable environments, and enabling reproducible research."
  },
  {
    "objectID": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#docker-and-biocontainers-running-bcftools-as-a-container",
    "href": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#docker-and-biocontainers-running-bcftools-as-a-container",
    "title": "Bioconda and Biocontainers Docker containers for bioinformatics",
    "section": "Docker and biocontainers: running bcftools as a container",
    "text": "Docker and biocontainers: running bcftools as a container\nIn some cases tools are already available from public container repositories - a good example of this is bcftools in the biocontainers container registry - if you read the documentation you will see that biocontainers is tightly tied to bioconda, but I’ll leave that to the reader to look into and here I’ll focus on the practical steps to use the tool.\nAfter selecting a docker container version from the bcftools page, it can be run as follows (we have chosen the image quay.io/biocontainers/bcftools:1.21--h8b25389_0):\n\ndocker pull quay.io/biocontainers/bcftools:1.21--h8b25389_0\ndocker run quay.io/biocontainers/bcftools:1.21--h8b25389_0 bcftools --version\n\nNote that after the docker run &lt;image&gt;, the command starts that you want to run in the shell. This is a common pattern.\nIf you are familiar with docker and containerization then you will be aware that this has executed a command in an isolated container on your local machine. Container resources do not automatically scale to your system resources, there are usually limits enforced by the docker client. You will need to manage allowed resources, for instance memory limits, via the docker client on your machine to ensure that enough memory is made available (for Mac and Windows you can do this via the docker desktop client).\nTo make use of this tooling then, we need to ensure the input and output directories are mounted to the container. This is done with a -v local-path:container-path argument to the container, mounting the local directory (specified here as local-path to the directory in the container container-path).\nIn its simplest form, to run a command in a docker container on the current working directory, we can mount the working directory as follows:\n\ndocker run -v \"${PWD}:/tmp/\" quay.io/biocontainers/bcftools:1.21--h8b25389_0 ls /tmp/\n\nNote that this is not running bcftools, it is running the command ls /tmp/ on an isolated (containerized) linux OS running on the local machine. If you run this command you will see a list of the files in the current working directory. Since bcftools is installed and in the path on the container, you can run bcftools on files in your working directory as below. In this example, we list the samples in a vcf file in the current working directory.\n\ndocker run \\\n    -v \"${PWD}:/tmp/\" \\\n    quay.io/biocontainers/bcftools:1.21--h8b25389_0 \\\n    bcftools query -l /tmp/test-file-chr22.vcf.gz &gt; /tmp/sample-id-list.txt\n\nThe command bcftools query -l /tmp/test-file-chr22.vcf.gz &gt; /tmp/sample-id-list.txt will have written the list of sample IDs to a file sample-id-list.txt to the /tmp/ directory on the container, so that now exists on your machine in the current working directory.\nNow, to understand this a little better: ${PWD} contains the path to you working directory (if you’re not familiar with this, type echo \"${PWD}\" into the command line and it will print the path to your current directory, usually achieved with pwd). We can thus generalize the above to mount any directory to the docker container. If you wanted to mount an input and an output directory, you could do this as follows:\n\ndocker run \\\n    -v \"/path/to/input/directory:/tmp/input/\" \\\n    -v \"/path/to/output/directory:/tmp/output/\" \\\n    quay.io/biocontainers/bcftools:1.21--h8b25389_0 \\\n    bcftools query -l /tmp/input/test-file-chr22.vcf.gz &gt; /tmp/output/sample-id-list.txt\n\nNotice that we have mounted the input directory to /tmp/input and the output directory to /tmp/output. These are updated accordingly in the bcftools command that is being run on the container.\n\nReproducing your results:\nOne major advantage of using docker containers is that you can simply rerun the commands to pull the same image and run the same commands. Two key points stand out:\n\nmake sure you script the commands you use, and\nbe wary of the latest tag: this is great for exploratory work but, as the name suggests, points to the latest version of the container image, and this will change over time."
  },
  {
    "objectID": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#footnotes",
    "href": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#footnotes",
    "title": "Bioconda and Biocontainers Docker containers for bioinformatics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWorkflow management tools will be the subject of a future post - I will likely post on nextflow and snakemake)↩︎"
  },
  {
    "objectID": "posts/003--planned-future-posts/upcoming-posts.html",
    "href": "posts/003--planned-future-posts/upcoming-posts.html",
    "title": "Planned/upcoming posts",
    "section": "",
    "text": "I plan to write on a number of topics, more or less documenting useful tools and methods I’ve come across or developed over the years. Most of this material is expanding notes I’ve taken or things that have proven useful to others when mentoring or teaching in the past. Some posts planned for a later date are as follows:\n\n[] Collected resources and strategies for\n\nlearning and becoming effective in R\n[] learning and becoming effective in Python\npopulation genetics resources\n\n[] DataFrames: base R, tibbles, data.table, Pandas and Polars - my perspective coming from R to the python ecosystem.\n[] Reflections on software engineering for research data science vs. production code: lessons learned.\n[] Building R packages - an introduction\n[] Optimizing R code with C++: Rcpp, RcppEigen, and RcppArmadillo by example.\n[] Incorporating python code into an R package.\n[] Rcpp Modules, exporting C++ classes to R.\n[] A repository structure for developing alembic migrations (docker + postgresql + python)\n[] Using python cookiecutter to setup containerized python scripts\n[] Working with parquet files in R and python\n[] Exploring the effect of normalization choices on PCA in population genetics\n[] Getting started with nextflow for workflow orchestration\n[] plotting and figure arrangement: R with ggplot2, and migrating this to python with plotnine\n[] plotting in python with Polars and Altair\n[] data validation with Pydantic\n[] handling dates and datetimes in R and python\n[] managing categorical variables (factors) in R\n[] Handling the command line: shell commands - a useful toolbox\n\n[] grep, rg, sed, awk, jq,\n[] reproducibility with docker and biocontainers\n[] AWS: awscli, aws-vault\n\n[] data wrangling and exploration with\n\n[] polars in Python\n[] pandas in Python\n[] dplyr in R\n\n[] using tidymodels with a bioinformatics application\n[] using scikit-learn with a bioinformatics application\n[] managing environments:\n\n[] conda/mamba\n[] rig + renv\n[] pyenv + venv/poetry\n\n[] Running jupyter lab remotely to access over an ssh connection\n[] permutations and group actions\n[] binomial coefficients and p-adic expansions\n\nAt some point I will post a little more on popular mathematics."
  },
  {
    "objectID": "posts/001--renv-project-structure/2024-10-29--my-first-post.html",
    "href": "posts/001--renv-project-structure/2024-10-29--my-first-post.html",
    "title": "Project creation - encouraging reproducibility with renv",
    "section": "",
    "text": "Since I’m returning to blogging after a number of years, I thought as a first post I’d start with a tool I recently wrote to address something I consider fundamental, but which I frequently find myself advocating for and providing training in - reproducibility in research1.\nThis will be a short post as, more specifically, I’ll briefly discuss a useful script I recently developed to setup an R analysis project with renv. The only requirement is a functioning R installation and jupyter-client.\nThe key reason for developing this script is to automate:\n\ncreation of a common project folder structure.\ncreation of an renv environment ready to use.\ninstallation of an R jupyter kernel ready for use.\n\nWhile this is a good start, it’s important to note that renv does not cover all aspects of reproducibility - some caveats are noted in the renv documentation, but critically if used correctly it does capture package dependency versions and the R version. Since I now frequently manage cloud compute environments using infrastructure-as-code tools such as terraform, and I can define start-up scripts to install dependencies such as R versions, this script is sufficient for reproducibility.\nThe script requires a kernel-name and project folder location as arguments and carries out the following steps:\n\nConstructs an analysis-ready folder structure. This is quite opinionated, but it has proven useful:\n\n/path/to/project/directory/\n├── data\n│   ├── 00_source\n│   ├── 01_staging\n│   └── 02_final\n├── figures\n├── notebooks\n└── output\n    ├── data\n    ├── figures\n    └── other\n\nInitialises an renv environment in the project root directory.\nInstalls the IRkernel package and installs the corresponding kernel for the user.\nCreates an R script to reinstall the kernel for the user in the notebooks folder.\nEnsures the renv environment is active in the notebooks directory. This is achieved by creating a particular .Rprofile file.\n\nThe final project directory structure is as follows (having removed all of the additional files installed in the renv/ folder):\n/path/to/project/directory/\n├── data\n│   ├── 00_source\n│   ├── 01_intermediate\n│   └── 02_processed\n├── figures\n├── notebooks\n│   └── 00-install-kernel.R\n├── output\n│   ├── data\n│   ├── figures\n│   └── other\n├── renv/...\n└── renv.lock\nI have found such a structure useful as it ensures\n\nEase of use for others:\n\nPerhaps most importantly it is portable, this project can be handed to another user/colleague and it is relatively easy to understand, assuming your code is well-written and naming conventions are sensible - this may be the subject of another post 2.\nHaving introduced a similar setup script like this to others, I have found it eases the transition from writing R code to creating reproducible analyses. This is particularly true for those new to computational analysis or those who are not computational scientists and engineers.\n\n\n\nEase of use for me:\n\nThe existence of the folders is a useful reminder how to keep the output and working files organised.\nThe only rule the user needs to remember is to keep all running code in the notebooks directory and to run renv::snapshot() periodically.\nThis is much faster than conda, and after both my own experience and supporting several users using conda, I have found the conda dependency management for R packages to be unreliable at times.\n\n\nThe immediate availability of the jupyter kernel is another key efficiency here, following project setup I can immediately open jupyter notebooks with the relevant R kernel in the notebooks directory.\nNote that it can also be extended to python virtual environments or conda managed python by keeping all environment configuration in the root directory (for example requirements.txt/environment.yml files, venv folders).\n\n\nThis was both the biggest surprise when I started using renv, and the trickiest part to get right in this project structure. As noted, I have configured this script to ensure that any R session started in the notebooks folder will use the renv stored in the root directory. This is not the case out-of-the-box with renv, but is enabled by creating a .Rprofile file in the notebooks directory containing the following line (as described in a GitHub issue here):\nowd &lt;- setwd(\"..\"); source(\"renv/activate.R\"); setwd(owd)\nWhen activated, the renv configuration must be stored in the working directory - this .Rprofile changes the working directory to enable the activation. The .Rprofile file is run when an R session is started (see here for more details about renv, and good pointers to the flexible but relatively complex configuration options for R). This file essentially changes the working directory to the root (parent) directory, activates the renv R environment there, and changes directory back to the notebooks directory again.\nI hope this script is useful to others - it has proven invaluable for me, and I expect to optimize it and add more features/options in future. It is publicly available on GitHub here."
  },
  {
    "objectID": "posts/001--renv-project-structure/2024-10-29--my-first-post.html#enabling-renv-in-the-notebooks-folder",
    "href": "posts/001--renv-project-structure/2024-10-29--my-first-post.html#enabling-renv-in-the-notebooks-folder",
    "title": "Project creation - encouraging reproducibility with renv",
    "section": "",
    "text": "This was both the biggest surprise when I started using renv, and the trickiest part to get right in this project structure. As noted, I have configured this script to ensure that any R session started in the notebooks folder will use the renv stored in the root directory. This is not the case out-of-the-box with renv, but is enabled by creating a .Rprofile file in the notebooks directory containing the following line (as described in a GitHub issue here):\nowd &lt;- setwd(\"..\"); source(\"renv/activate.R\"); setwd(owd)\nWhen activated, the renv configuration must be stored in the working directory - this .Rprofile changes the working directory to enable the activation. The .Rprofile file is run when an R session is started (see here for more details about renv, and good pointers to the flexible but relatively complex configuration options for R). This file essentially changes the working directory to the root (parent) directory, activates the renv R environment there, and changes directory back to the notebooks directory again.\nI hope this script is useful to others - it has proven invaluable for me, and I expect to optimize it and add more features/options in future. It is publicly available on GitHub here."
  },
  {
    "objectID": "posts/001--renv-project-structure/2024-10-29--my-first-post.html#footnotes",
    "href": "posts/001--renv-project-structure/2024-10-29--my-first-post.html#footnotes",
    "title": "Project creation - encouraging reproducibility with renv",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ncomputational reproducibility should not be confused with the so-called reproducibility crisis in research, which spans far beyond only being able to reproduce results from your code and a fixed dataset. However, computational reproducibility is essential for the credibility of results - if you can’t reproduce your computational results from the source data, you should not trust them or publish them. This has a strong implication for record-keeping, and version control for both code and datasets.↩︎\nRegarding naming conventions - personally I name wrapper scripts/notebooks/workflows with zero-padded integers followed by meaningful names to ensure it is clear in which order they are run, and place utility scripts and executables in a bin/ folder. I use linters for R code and linters and autoformatters (usually black or ruff) for python. I prefer to use a coding standard, both for consistency, and to ensure that my focus is on code content. For R I (and Google) recommend the tidyverse code style. If you’re not familiar with these, it may be worth reading the tidyverse style guide and Google’s additions.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is still in-development, and will replace my previous personal site. Over time I will migrate my old site to this one, expanding my personal pages and blog posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "How To Recursively chmod\n\n\n\n\n\n\nbash\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with large datasets with Arrow\n\n\n\n\n\n\nR\n\n\narrow\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nDec 15, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nGenetics Resources\n\n\n\n\n\n\nresources\n\n\ngenetics\n\n\ndata-science\n\n\n\n\n\n\n\n\n\nNov 23, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nBioconda and Biocontainers Docker containers for bioinformatics\n\n\n\n\n\n\ndocker\n\n\nbioinformatics\n\n\ncontainerization\n\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with a local postgres database docker container\n\n\n\n\n\n\ndata-analysis\n\n\ndata-engineering\n\n\nSQL\n\n\nintroductory\n\n\ndocker\n\n\ncontainerization\n\n\n\nSetting up and connecting to a local postgresql docker container\n\n\n\n\n\nOct 31, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nPlanned/upcoming posts\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nResources for learning R\n\n\n\n\n\n\nresources\n\n\nR\n\n\ndata-science\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nProject creation - encouraging reproducibility with renv\n\n\n\n\n\n\ndata-science\n\n\nreproducible\n\n\nR\n\n\n\nAutomating renv project creation\n\n\n\n\n\nOct 29, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nCG\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html",
    "href": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html",
    "title": "Working with a local postgres database docker container",
    "section": "",
    "text": "In this post I’ll describe some basic commands to work locally with a postgresql database in a docker container. There are many advantages to this - you can run it as an ephemeral database, or build and keep a container image with your data in it. Although the commands and details here are basic, I’ve found this to be a great resource for testing out queries or query patterns."
  },
  {
    "objectID": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html#starting-and-stopping-a-postgres-container-database",
    "href": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html#starting-and-stopping-a-postgres-container-database",
    "title": "Working with a local postgres database docker container",
    "section": "Starting and stopping a postgres container database",
    "text": "Starting and stopping a postgres container database\nTo start the container, I first pull the latest image as follows\n\ndocker pull postgres\n\nNext, I use the following command, with suitable substitutes for the username “postgres” and password “mypassword” specified here.\n\ndocker run --rm -d -p 5432:5432 -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=mypassword --name tmp-postgres postgres\n\nRunning the following command will show the container is running.\n\ndocker container ls\n\nCONTAINER ID   IMAGE      COMMAND                  CREATED         STATUS                  PORTS                    NAMES\nfbb62930e7ff   postgres   \"docker-entrypoint.s…\"   2 seconds ago   Up Less than a second   0.0.0.0:5432-&gt;5432/tcp   tmp-postgres\n\n\n\nYou now have a local postgres database running on your machine. It is useful to understand the various options specified in the above command. This is an expansion of a simpler command docker run postgres, with some configuration options.\n\n-d: runs the container detached from your shell;\n--rm: This option ensures that data is not stored in the container when stopped;\n-p 5432:5432: This is port forwarding port 5432 to localhost;\n-e POSTGRES_USER=postgres: sets up a database user with username postgres;\n-e POSTGRES_PASSWORD=password : sets the password for the above user to be password;\n--name tmp-postgres: sets the container name while it is running.\n\nTo stop the container, simply run the following command\n\ndocker stop tmp-postgres"
  },
  {
    "objectID": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html#connecting-to-the-local-database",
    "href": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html#connecting-to-the-local-database",
    "title": "Working with a local postgres database docker container",
    "section": "Connecting to the local database",
    "text": "Connecting to the local database\nI will write separate posts concerning connecting to the database with R, Python elsewhere, but will start here with visual studio code.\n\nVisual Studio Code\nI have found the SQLTools extension to be sufficient, with the SQLTools PostgreSQL/Cockroach extension. Once installed, open the extension from the icon, select create a new connection, choose the PostreSQL driver, and the following options:\n\nConnect using: Server and Port\nServer Address: localhost\nPort: 5432\nDatabase: postgres\nUsername: postgres (the username specified earlier)\nPassword: mypassword (Select save as plain text in settings, or ask on connect and when required enter the password specified earlier)\n\nI may add further details and screenshots here at a later date."
  },
  {
    "objectID": "posts/004--R-resources/R-resource.html",
    "href": "posts/004--R-resources/R-resource.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "This is a collection of resources I have found over the years that I find myself returning to or recommending to others.\nR is described as “a free software environment for statistical computing and graphics” at the R project for Statistical Computing. For me the strength of R is not only as a programming language but also a powerful statistical computing system, it is relatively easy to use for statistical analysis and has excellent facilities to easily produce publication quality plots.\nComing from a background in pure mathematics, my first exposure to coding and software development was in MatLab, MAGMA and GAP. I later branched out into C, C++ and Python. My exposure to MAGMA and GAP was mainly in algebraic and combinatoric algorithms, but it was in R that I first really got to grips with statistical computing. Over the years I have written several R packages for data visualisation tools, tools to analyse electronic health data, and developed highly optimized R packages providing statistical machine learning algorithms for the analysis of large scale single cell RNA-seq datasets. I have optimized my R code extensively, integrated C++ code, made use of Rmarkdown (and now quarto) for data analysis and building websites, and it is still my primary tool for data visualisation. I have also taught students to use R over several years, so hopefully the resources I point to here will be useful."
  },
  {
    "objectID": "posts/004--R-resources/R-resource.html#free-resources",
    "href": "posts/004--R-resources/R-resource.html#free-resources",
    "title": "Resources for learning R",
    "section": "Free Resources",
    "text": "Free Resources\n\nData Science and software development\nHadley Wickham and the posit team have produced some excellent resources, which tend to promote their tidyverse tooling.\n\nR for Data Science: The introductory book from Hadley Wickham, an excellent resource for beginners and intermediate users alike.\nR packages: Hadley WIckham and Jennifer Bryan’s excellent introduction to building R packages.\nAdvanced R: An advanced book from Hadley Wickham - I thoroughly enjoyed it, the content is excellent and well written, but is for established R users.\nWhat they forgot to teach you about R Although still a work in progress, this site has some very useful context on how R works and also generally good advice on tried-and-tested good habits to adopt.\n\nRoger Peng has produced a handy book covering some aspects of software development in R. This is excellent if you’re serious about developing packages, but may not be for you if you are mostly interested in analysis.\n\nMastering Software Development with R: a very useful book from Roger Peng and others."
  },
  {
    "objectID": "posts/004--R-resources/R-resource.html#introductory-bookscourses",
    "href": "posts/004--R-resources/R-resource.html#introductory-bookscourses",
    "title": "Resources for learning R",
    "section": "Introductory books/courses",
    "text": "Introductory books/courses\nThe excellent R for Data Science book referenced above is one of many good introductory books and courses available, all of which have a different perspective.\nThe following books start more or less “from scratch”, so are excellent if you’re just getting started.\n\nData Analysis for the Life Sciences: a useful introductory book from Rafael Irizarry\nIntroduction to Data Science: Data Analysis and Prediction Algorithms with R: another book from Rafael Irizarry\nExploratory Data Analysis with R Roger Peng’s introductory book on exploratory data analysis. Some of the chapters are a little brief, but it has several good starting points.\n\nRobin Evans’ University of Oxford Statistical Computing course\n\nIntegrating C++ with R through Rcpp\nIn some cases, optimizing your R code can only go so far, and some lower-level programming is required. I’ve found the Rcpp suite of packages very helpful in automating the build and bindings to expose C++ code in my R packages. Particularly Rcpp, RcppArmadillo and RcppEigen.\n\nRcpp\nRcpp gallery\n\n\n\nVisualisation\nR is capable of producing beautiful publication-ready figures. While I don’t subscribe to the notion that one tool is inherently better than any others, I find the ggplot2 library and in particular the grammar of graphics expressive and efficient. This is particularly helpful during exploratory analyses, helping to iterate quickly.\n\nIntroduction to ggplot2: an introduction to ggplot2 from the tidyverse team\nAn introduction to ggplot2: a useful introduction with examples.\nFundamentals of Data Visualisation an excellent introduction from cowplot developer Claus O. Wilke\nggplot2 book : not the best place to start for beginners, but worth reading once familiar with the basics (if you are a beginner, stick to R for Data Science for the basics).\nR graph gallery this is an excellent resource, particulary when getting started.\n\n\n\nCode Style\n\nThe tidyverse style guide Personally, I have found this an excellent resource, and comparison with the Google style guide was time well-spent.\n\n\n\nOther useful resources\n\nThe Arrow R cookbook an excellent cookbook of approaches to use arrow tooling in R.\nR Manuals Again, this is rather a lot of detail if you are starting out, but for an advanced user it can be a useful resource.\nWriting R extensions This is an excellent resource once you’re established with building R packages. To get started I recommend the book by Hadley Wickham and Jennifer Bryan linked above.\n\n\n\nSelected blogs and resources\n\nThe R blog for discussions about upcoming features in R. Probably for advanced users.\n\nSeveral of these links I return to time and time again, so I am sure I will return to this page to dig them out in the near future."
  },
  {
    "objectID": "posts/006-genetics-resources/genetics-resources.html",
    "href": "posts/006-genetics-resources/genetics-resources.html",
    "title": "Genetics Resources",
    "section": "",
    "text": "I have been lucky to have worked in some diverse and fascinating research areas - i love to discuss it and to pass on some useful resources I’ve found. However, I rely on bookmarks and google far too much to find resources, so this post is intended to enable me and hopefully others to find some useful references.\nI was fortunate to work in the labs of Jonathan Marchini and Simon Myers on some aspects of population genetics and functional genomics. This post will focus on genetics resources, and initially mostly population genetics. As I have very limited time today I will add just a few free learning resources from population genetics and will later update with some of the others I have planned."
  },
  {
    "objectID": "posts/006-genetics-resources/genetics-resources.html#population-genetics-resources",
    "href": "posts/006-genetics-resources/genetics-resources.html#population-genetics-resources",
    "title": "Genetics Resources",
    "section": "Population Genetics Resources",
    "text": "Population Genetics Resources\n\nGraham Coop’s Population and Quantitative Genetics book - population genetics notes from the Coop lab, this is an immensely useful resource. I really appreciated the coverage of many aspects of the subject that are “well known”, and therefore no longer referenced, but often difficult to pin down for newcomers to the field1.\n\nThe Coop lab blog\n\n\nJonathan Pritchard’s An Owner’s Guide to the Human Genome: an introduction to human population genetics, variation and disease - an excellent resource from Jonathan Pritchard. The Pritchard lab page is well worth a visit.\n\nWhen I have time I will add in a recommended reading list of books I have enjoyed and that give a good grounding in population genetics, both empirical and theoretical."
  },
  {
    "objectID": "posts/006-genetics-resources/genetics-resources.html#footnotes",
    "href": "posts/006-genetics-resources/genetics-resources.html#footnotes",
    "title": "Genetics Resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think everyone who has worked in research and had to find their way around a new field will be familiar with this - that when a concept or technique is well-known enough it is no longer referenced in the literature (except PhD Theses or occasionally survey papers). The challenge is to discern when a concept is well-known enough that it no longer requires explanation or referencing in new publications. This is even more of a challenge in interdisciplinary work in which collaborating scientists have differing views of what is “well-known”.↩︎"
  },
  {
    "objectID": "posts/008-recursive-chmod/008-how-to-recursively-chmod.html",
    "href": "posts/008-recursive-chmod/008-how-to-recursively-chmod.html",
    "title": "How To Recursively chmod",
    "section": "",
    "text": "When working on a unix based system, the most common way to manage permissions is with the chmod command. While there is a -R recursive flag, this does not distinguish between files and directories. Since the solution to this has proven useful to several people recently, I thought I’d share it here.\nThe best way I’ve found to get around this is to use find and the -exec option to execute a command. For example, to recursively chmod all files in subdirectories of a directory named DATA/ to be read-only for all users (444) the following command will suffice.\nfind DATA/ -type f -exec chmod 444 \"{}\" \\;\nSimilarly, if you’d like to chmod 555 all subdirectories:\nfind DATA/ -type d -exec chmod 555 \"{}\" \\;\nTo break down these commands, the find DATA/ -type d command will recursively list all subdirectories of DATA/. The -exec part defines a command to run for all found directories. The parentheses are replaced by the directories found in the find command and escaping the semicolon ensures the command is parsed correctly."
  }
]