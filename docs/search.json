[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is still in-development, and will replace my previous personal site. Over time I will migrate my old site to this one, expanding my personal pages and blog posts."
  },
  {
    "objectID": "posts/003--planned-future-posts/upcoming-posts.html",
    "href": "posts/003--planned-future-posts/upcoming-posts.html",
    "title": "Planned/upcoming posts",
    "section": "",
    "text": "I plan to write on a number of topics, more or less documenting useful tools and methods I’ve come across or developed. Some posts planned for a later date are as follows:\n\nCollected resources and strategies for\n\nlearning and becoming effective in R\nlearning and becoming effective in Python\npopulation genetics resources\n\nDataFrames: base R, tibbles, data.table, Pandas and Polars - my perspective coming from R to the python ecosystem.\nReflections on software engineering for research data science vs. production code: lessons learned.\nBuilding R packages - an introduction\nOptimizing R code with C++ and Rcpp, RcppEigen, and RcppArmadillo by example.\nIncorporating python code into an R package.\nRcpp Modules, exporting C++ classes to R.\nA repository structure for developing alembic migrations (docker + postgresql + python)\nUsing python cookiecutter to setup containerized python scripts\nWorking with parquet files in R and python\nExploring the effect of normalization choices on PCA in population genetics\nGetting started with nextflow for workflow orchestration\nplotting and figure arrangement: R with ggplot2, and migrating this to python with plotnine\nplotting in python with Polars and Altair\ndata validation with Pydantic\nhandling dates and datetimes in R and python\nHandling the command line: shell commands - a useful toolbox\n\ngrep, rg, sed, awk, jq,\nreproducibility with docker and biocontainers\nAWS: awscli, aws-vault\n\ndata wrangling and exploration with\n\npolars in Python\npandas in Python\ndplyr in R\n\nusing tidymodels with a bioinformatics application\nusing scikit-learn with a bioinformatics application\nmanaging environments:\n\nconda/mamba\nrig + renv\npyenv + venv/poetry\n\nRunning jupyter lab remotely to access over an ssh connection\npermutations and group actions\nbinomial coefficients and p-adic expansions\n\nAt some point I will post a little more on popular mathematics."
  },
  {
    "objectID": "posts/001--renv-project-structure/2024-10-29--my-first-post.html",
    "href": "posts/001--renv-project-structure/2024-10-29--my-first-post.html",
    "title": "Project creation - encouraging reproducibility with renv",
    "section": "",
    "text": "Since I’m returning to blogging after a number of years, I thought as a first post I’d start with a tool I recently wrote to address something I consider fundamental, but which I frequently find myself advocating for and providing training in - reproducibility in research1.\nThis will be a short post as, more specifically, I’ll briefly discuss a useful script I recently developed to setup an R analysis project with renv. The only requirement is a functioning R installation and jupyter-client.\nThe key reason for developing this script is to automate:\n\ncreation of a common project folder structure.\ncreation of an renv environment ready to use.\ninstallation of an R jupyter kernel ready for use.\n\nWhile this is a good start, it’s important to note that renv does not cover all aspects of reproducibility - some caveats are noted in the renv documentation, but critically if used correctly it does capture package dependency versions and the R version. Since I now frequently manage cloud compute environments using infrastructure-as-code tools such as terraform, and I can define start-up scripts to install dependencies such as R versions, this script is sufficient for reproducibility.\nThe script requires a kernel-name and project folder location as arguments and carries out the following steps:\n\nConstructs an analysis-ready folder structure. This is quite opinionated, but it has proven useful:\n\n/path/to/project/directory/\n├── data\n│   ├── 00_source\n│   ├── 01_staging\n│   └── 02_final\n├── figures\n├── notebooks\n└── output\n    ├── data\n    ├── figures\n    └── other\n\nInitialises an renv environment in the project root directory.\nInstalls the IRkernel package and installs the corresponding kernel for the user.\nCreates an R script to reinstall the kernel for the user in the notebooks folder.\nEnsures the renv environment is active in the notebooks directory. This is achieved by creating a particular .Rprofile file.\n\nThe final project directory structure is as follows (having removed all of the additional files installed in the renv/ folder):\n/path/to/project/directory/\n├── data\n│   ├── 00_source\n│   ├── 01_staging\n│   └── 02_final\n├── figures\n├── notebooks\n│   └── 00-install-kernel.R\n├── output\n│   ├── data\n│   ├── figures\n│   └── other\n├── renv/...\n└── renv.lock\nI have found such a structure useful as it ensures\n\nEase of use for others:\n\nPerhaps most importantly it is portable, this project can be handed to another user/colleague and it is relatively easy to understand, assuming your code is well-written and naming conventions are sensible - this may be the subject of another post 2.\nHaving introduced a similar setup script like this to others, I have found it eases the transition from writing R code to creating reproducible analyses. This is particularly true for those new to computational analysis or those who are not computational scientists and engineers.\n\n\n\nEase of use for me:\n\nThe existence of the folders is a useful reminder how to keep the output and working files organised.\nThe only rule the user needs to remember is to keep all running code in the notebooks directory and to run renv::snapshot() periodically.\nThis is much faster than conda, and after supporting several users using conda, I have found the conda dependency management for R packages to be unreliable at times.\n\n\nThe immediate availability of the jupyter kernel is another key efficiency here, following project setup I can immediately open jupyter notebooks with the relevant R kernel in the notebooks directory.\nNote that it can also be extended to python virtual environments or conda managed python by keeping all environment configuration in the root directory (for example requirements.txt/environment.yml files, venv folders).\n\n\nThis was both the biggest surprise when I started using renv, and the trickiest part to get right in this project structure. As noted, any R session started in the notebooks folder will use the renv stored in the root directory. This is enabled by creating a .Rprofile file in the notebooks directory containing the following line (as described in a GitHub issue here):\nowd &lt;- setwd(\"..\"); source(\"renv/activate.R\"); setwd(owd)\nThe .Rprofile file is run when an R session is started (see here for more details about renv, and good pointers to the flexible but relatively complex configuration options for R). This file essentially changes the working directory to the root (parent) directory, activates the renv R environment there, and changes directory back to the notebooks directory again.\nI hope this script is useful to others - it has proven invaluable for me, and I expect to optimize it and add more features/options in future. It is publicly available on GitHub here."
  },
  {
    "objectID": "posts/001--renv-project-structure/2024-10-29--my-first-post.html#enabling-renv-in-the-notebooks-folder",
    "href": "posts/001--renv-project-structure/2024-10-29--my-first-post.html#enabling-renv-in-the-notebooks-folder",
    "title": "Project creation - encouraging reproducibility with renv",
    "section": "",
    "text": "This was both the biggest surprise when I started using renv, and the trickiest part to get right in this project structure. As noted, any R session started in the notebooks folder will use the renv stored in the root directory. This is enabled by creating a .Rprofile file in the notebooks directory containing the following line (as described in a GitHub issue here):\nowd &lt;- setwd(\"..\"); source(\"renv/activate.R\"); setwd(owd)\nThe .Rprofile file is run when an R session is started (see here for more details about renv, and good pointers to the flexible but relatively complex configuration options for R). This file essentially changes the working directory to the root (parent) directory, activates the renv R environment there, and changes directory back to the notebooks directory again.\nI hope this script is useful to others - it has proven invaluable for me, and I expect to optimize it and add more features/options in future. It is publicly available on GitHub here."
  },
  {
    "objectID": "posts/001--renv-project-structure/2024-10-29--my-first-post.html#footnotes",
    "href": "posts/001--renv-project-structure/2024-10-29--my-first-post.html#footnotes",
    "title": "Project creation - encouraging reproducibility with renv",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ncomputational reproducibility should not be confused with the so-called reproducibility crisis in research, which spans far beyond only being able to reproduce results from your code and a fixed dataset. However, computational reproducibility is essential for the credibility of results - if you can’t reproduce your computational results from the source data, you should not trust them or publish them. This has a strong implication for record-keeping, and version control for both code and datasets.↩︎\nRegarding naming conventions - personally I name wrapper scripts/notebooks/workflows with zero-padded integers followed by meaningful names to ensure it is clear in which order they are run, and place utility scripts and executables in a bin/ folder. I use linters for R code and linters and autoformatters (usually black or ruff) for python. I prefer to use a coding standard, both for consistency, and to ensure that my focus is on code content. For R I (and Google) recommend the tidyverse code style. If you’re not familiar with these, it may be worth reading the tidyverse style guide and Google’s additions.↩︎"
  },
  {
    "objectID": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html",
    "href": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html",
    "title": "Bioconda and Docker containers for bioinformatics",
    "section": "",
    "text": "This post is intended to give a short practical example of using two key tools to install bioinformatics tools and use them. Both conda and docker are widely used. I have used conda for a little under a decade and only really adopted docker a few years ago, but I can see that containerization is more and more widely used. Hopefully this post can serve as a practical example for those experimenting with these approaches.\nI find tools like conda, pyenv, pip, venv, rig, and renv are all excellent for environment management. They all have a learning curve, but quickly become part of your day-to-day tooling. However, the problem remains that sometimes you are at the start of a new project, or worse halfway through an existing project and you need to install a new tool - this new tool may have new dependencies and in the latter case, may well share dependencies with existing tools, potentially with different versions. The unfortunate truth of the matter is many bioinformaticians spend an unreasonable amount of time getting their tooling setup.\nThere are many solutions to this - one I have seen many resort to, is to have a conda environment per tool, to ensure dependencies are handled independently. Whether you use a separate environment for each tool or not, using conda is effective and I will give some basic commands to get started in the first part of this post. An alternative approach is to use pre-built docker containers, which I will demonstrate in the second half of this post."
  },
  {
    "objectID": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#using-bioconda-to-install-bcftools",
    "href": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#using-bioconda-to-install-bcftools",
    "title": "Bioconda and Docker containers for bioinformatics",
    "section": "Using Bioconda to install bcftools",
    "text": "Using Bioconda to install bcftools\nThe bioconda channel contains lots of useful tooling for bioinformatics, meaning it’s possible to use conda to manage some bioinformatics tooling directly. To add the bioconda channel to you conda setup, the bioconda documentation suggests to run the following commands to update your ~/.condarc file.\n\nAdd bioconda to the conda channels\n\n\n    conda config --add channels bioconda\n    conda config --add channels conda-forge\n    conda config --set channel_priority strict\n\n\n    # list activate channels as follows\n    conda config --show channels\n\n\nCreate and activate your conda environment as usual\n\n\n    conda create -n bcftools-env -y\n    conda activate bcftools-env\n\n\ninstall bcftools\n\n\n    conda install -c bioconda bcftools -y\n\n\nrun bcftools\n\n\n    bcftools --version\n\n\nOnce you have a working environment that you can carry out your analysis in, don’t forget to export the environment yml\n\n\n    conda env export &gt; environment.yml\n\nBioconda has gone a long way to remove the overhead of managing all the environment dependencies, understanding the dependencies of all your tools and compiling the source code. I have used the above setup on my personal notebook and also on remote compute environments when working in the cloud. However, conda environments are somewhat slow to setup (admittedly of the order of minutes, or at worst hours, not days, usually)\n\nReproducing your results: recreating your conda environment\nNote: if you, or a colleague/collaborator later want to recreate an environment to rerun an analysis, you can do so using the environment.yml. The following command: conda env create -f environment.yml will recreate the environment. This is a really powerful tool for creating portable environments, and enabling reproducible research."
  },
  {
    "objectID": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#docker-and-biocontainers-running-bcftools-as-a-container",
    "href": "posts/005-docker-container-for-bioinformatics/docker-containers-for-bioinformatics.html#docker-and-biocontainers-running-bcftools-as-a-container",
    "title": "Bioconda and Docker containers for bioinformatics",
    "section": "Docker and biocontainers: running bcftools as a container",
    "text": "Docker and biocontainers: running bcftools as a container\nIn some cases tools are already available from public container repositories - a good example of this is bcftools in the biocontainers container registry. After selecting a docker container version from the bcftools page, it can be run as follows (we have chosen the image quay.io/biocontainers/bcftools:1.21--h8b25389_0):\n\ndocker pull quay.io/biocontainers/bcftools:1.21--h8b25389_0\ndocker run quay.io/biocontainers/bcftools:1.21--h8b25389_0 bcftools --version\n\nNote that after the docker run &lt;image&gt;, the command starts that you want to run in the shell. This is a common pattern.\nIf you are familiar with docker and containerization then you will be aware that this has executed a command in a container on your local machine. Note that machine resources do not automatically scale to your system, there are usually limits enforced by the docker client. You will need to manage allowed resources, for instance memory limits, via the docker client on your machine to ensure that enough memory is made available (for Mac and Windows you can do this via the docker desktop client).\nTo make use of this tooling then, we need to ensure the input and output directories are mounted to the container. This is done with a -v local-path:container-path argument to the container.\nIn its simplest form, to run a docker container on the current location, we can mount the working directory as follows:\n\ndocker run -v \"${PWD}:/tmp/\" quay.io/biocontainers/bcftools:1.21--h8b25389_0 ls /tmp/\n\nNote that this is not running bcftools, it is running the command ls /tmp/ on an isolated (containerized) linux OS running on the local machine. If you run this command you will see a list of the files in the current working directory. Since bcftools is installed and in the path on the container, you can run bcftools on files in your working directory as below. In this example, we list the samples in a vcf file in the current working directory.\n\ndocker run \\\n    -v \"${PWD}:/tmp/\" \\\n    quay.io/biocontainers/bcftools:1.21--h8b25389_0 \\\n    bcftools query -l /tmp/test-file-chr22.vcf.gz &gt; /tmp/sample-id-list.txt\n\nThe command bcftools query -l /tmp/test-file-chr22.vcf.gz &gt; /tmp/sample-id-list.txt will have written the list of sample IDs to a file sample-id-list.txt that now exists on your machine in the current working directory.\nNow, ${PWD} contains the path to you working directory (if you’re not familiar with this, type echo \"${PWD}\" into the command line and it will print the path to your current directory, usually achieved with pwd). We can thus generalize the above to mount any directory to the docker container. If you wanted to mount an input and an output directory, you could do this as follows:\n\ndocker run \\\n    -v \"/path/to/input/directory:/tmp/input/\" \\\n    -v \"/path/to/output/directory:/tmp/output/\" \\\n    quay.io/biocontainers/bcftools:1.21--h8b25389_0 \\\n    bcftools query -l /tmp/input/test-file-chr22.vcf.gz &gt; /tmp/output/sample-id-list.txt\n\nNotice that we have mounted the input directory to /tmp/input and the output directory to /tmp/output. The use of the directories in the bcftools command that is executed is updated accordingly.\n\nReproducing your results:\nOne major advantage with using docker containers is that you can simply rerun the commands to pull the same image and run the same commands (make sure you script the commands you use)."
  },
  {
    "objectID": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html",
    "href": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html",
    "title": "Working with a local postgres database docker container",
    "section": "",
    "text": "In this post I’ll describe some basic commands to work locally with a postgresql database in a docker container. There are many advantages to this - you can run it as an ephemeral database, or build and keep a container image with your data in it. Although the commands and details here are basic, I’ve found this to be a great resource for testing out queries or query patterns."
  },
  {
    "objectID": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html#starting-and-stopping-a-postgres-container-database",
    "href": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html#starting-and-stopping-a-postgres-container-database",
    "title": "Working with a local postgres database docker container",
    "section": "Starting and stopping a postgres container database",
    "text": "Starting and stopping a postgres container database\nTo start the container, I first pull the latest image as follows\n\ndocker pull postgres\n\nNext, I use the following command, with suitable substitutes for the username “postgres” and password “mypassword” specified here.\n\ndocker run --rm -d -p 5432:5432 -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=mypassword --name tmp-postgres postgres\n\nRunning the following command will show the container is running.\n\ndocker container ls\n\nCONTAINER ID   IMAGE      COMMAND                  CREATED        STATUS                  PORTS                    NAMES\nf493710380fe   postgres   \"docker-entrypoint.s…\"   1 second ago   Up Less than a second   0.0.0.0:5432-&gt;5432/tcp   tmp-postgres\n\n\n\nYou now have a local postgres database running on your machine. It is useful to understand the various options specified in the above command. This is an expansion of a simpler command docker run postgres, with some configuration options.\n\n-d: runs the container detached from your shell;\n--rm: This option ensures that data is not stored in the container when stopped;\n-p 5432:5432: This is port forwarding port 5432 to localhost;\n-e POSTGRES_USER=postgres: sets up a database user with username postgres;\n-e POSTGRES_PASSWORD=password : sets the password for the above user to be password;\n--name tmp-postgres: sets the container name while it is running.\n\nTo stop the container, simply run the following command\n\ndocker stop tmp-postgres"
  },
  {
    "objectID": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html#connecting-to-the-local-database",
    "href": "posts/002--postgresql-in-docker/2024-10-31--working-with-a-local-postgresql-database.html#connecting-to-the-local-database",
    "title": "Working with a local postgres database docker container",
    "section": "Connecting to the local database",
    "text": "Connecting to the local database\nI will write separate posts concerning connecting to the database with R, Python elsewhere, but will start here with visual studio code.\n\nVisual Studio Code\nI have found the SQLTools extension to be sufficient, with the SQLTools PostgreSQL/Cockroach extension. Once installed, open the extension from the icon, select create a new connection, choose the PostreSQL driver, and the following options:\n\nConnect using: Server and Port\nServer Address: localhost\nPort: 5432\nDatabase: postgres\nUsername: postgres (the username specified earlier)\nPassword: mypassword (Select save as plain text in settings, or ask on connect and when required enter the password specified earlier)\n\nI may add further details and screenshots here at a later date."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nThis is the first time I have returned to blogging in some years, and I look forward to it. I hope to repost some of my old work as well as create new material. The posts are at times notes for myself, with tools I’ve developed or used, as well as opinions, pedagogical posts, tips, tricks, and references. I have learnt a great deal from reading other blogs, so I hope the posts generated in this blog are of similar help to others."
  },
  {
    "objectID": "posts/004--R-resources/R-resource.html",
    "href": "posts/004--R-resources/R-resource.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "R is described as “a free software environment for statistical computing and graphics” at the R project for Statistical Computing. For me the strength of R is not only as a programming language but also a powerful statistical computing system, it is powerful, relatively easy to use for statistical analysis and has excellent facilities to easily produce publication quality plots.\nComing from a background in pure mathematics, my first exposure to coding and software development was in MatLab, MAGMA and GAP. I later branched out into C, C++ and Python. My exposure to MAGMA and GAP was mainly in algebraic and combinatoric algorithms, but it was in R that I first really got to grips with statistical computing. Over the years I have written several R packages for data visualisation tools, tools to analyse electronic health data, and developed highly optimized R packages providing statistical machine learning algorithms for the analysis of large scale single cell RNA-seq datasets. I have optimized my R code extensively, integrated C++ code, made use of Rmarkdown (and now quarto) for data analysis and building websites, and it is still my primary tool for data visualisation. I have also taught students to use R over several years, so hopefully the resources I point to here will be useful."
  },
  {
    "objectID": "posts/004--R-resources/R-resource.html#free-resources",
    "href": "posts/004--R-resources/R-resource.html#free-resources",
    "title": "Resources for learning R",
    "section": "Free Resources",
    "text": "Free Resources\n\nSoftware Development and Data Science\nHadley Wickham and the posit team have produced some excellent resources, which tend to promote their tidyverse tooling.\n\nR for Data Science: The introductory book from Hadley Wickham, an excellent resource for beginners and intermediate users alike.\nR packages: Hadley WIckham and Jennifer Bryan’s excellent introduction to building R packages.\nAdvanced R: An advanced book from Hadley Wickham - I thoroughly enjoyed it, the content is excellent and well written, but is for established R users.\n\nRoger Peng has produced a handy book re. software development in R. This is excellent if you’re serious about developing packages, but may not be for you if you are mostly interested in analysis. - Mastering Software Development with R: a very useful book from Roger Peng and others.\nRafael Irizarry has also written two introductory courses, which start from scratch but equip you to carry out many statistical analyses in R. - Data Analysis for the Life Sciences: a useful introductory book from Rafael Irizarry - Introduction to Data Science: Data Analysis and Prediction Algorithms with R: another book from Rafael Irizarry\nRobin Evans’ University of Oxford Statistical Computing course\n\n\nIntegrating C++ with R through Rcpp\n\nRcpp\nRcpp gallery\n\n\n\nVisualisation\n\nIntroduction to ggplot2: an introduction to ggplot2 from the tidyverse team\nAn introduction to ggplot2: a useful introduction with examples.\nFundamentals of Data Visualisation an excellent introduction from cowplot developer Claus O. Wilke\nggplot2 book : not the best place to start for beginners, but worth reading once familiar with the basics (if you are a beginner, stick to R for Data Science for the basics).\n\n\n\nOther useful resources\n\nThe R blog for discussions about upcoming features in R. Probably for advanced users.\nR Manuals Again, this is rather a lot of detail if you are starting out, but for an advanced user it can be a useful resource.\nWriting R extensions This is an excellent resource once you’re established with building R packages. To get started I recommend the book by Hadley Wickham and Jennifer Bryan linked above.\n\nSeveral of these links I return to time and time again, so I am sure I will return to this page to dig them out in the near future."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Bioconda and Docker containers for bioinformatics\n\n\n\n\n\n\ndocker\n\n\nbioinformatics\n\n\ncontainerization\n\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with a local postgres database docker container\n\n\n\n\n\n\ndata-analysis\n\n\ndata-engineering\n\n\nSQL\n\n\nintroductory\n\n\ndocker\n\n\ncontainerization\n\n\n\nSetting up and connecting to a local postgresql docker container\n\n\n\n\n\nOct 31, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nResources for learning R\n\n\n\n\n\n\nresources\n\n\nR\n\n\ndata-science\n\n\nsoftware-development\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nPlanned/upcoming posts\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 31, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nProject creation - encouraging reproducibility with renv\n\n\n\n\n\n\ndata-science\n\n\nreproducible\n\n\nR\n\n\n\nAutomating renv project creation\n\n\n\n\n\nOct 29, 2024\n\n\nCG\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nOct 26, 2024\n\n\nCG\n\n\n\n\n\n\nNo matching items"
  }
]